{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/raaed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/raaed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/raaed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "bow_vectorizer = CountVectorizer()\n",
    "brnli_vectorizer = CountVectorizer(binary = True)\n",
    "add_one = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath) -> list[tuple]:\n",
    "    ham_data = []\n",
    "    spam_data = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(filepath):\n",
    "        for filename in filenames:\n",
    "            with open(os.path.join(dirpath,filename), encoding = 'iso-8859-1') as f:\n",
    "                if 'ham' in filename:\n",
    "                    ham_data.append(f.read())\n",
    "                elif 'spam' in filename:\n",
    "                    spam_data.append(f.read())\n",
    "    \n",
    "    # assign 1/0 to samples and combine data\n",
    "    positive_samples = [(email, 1) for email in ham_data]\n",
    "    negative_samples = [(email, 0) for email in spam_data]\n",
    "\n",
    "    all_samples = positive_samples + negative_samples\n",
    "    random.shuffle(all_samples)\n",
    "\n",
    "    #remove stop words and lemmatize dataset\n",
    "    filtered_dataset = []\n",
    "    for text, y in all_samples:\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words = [word for word in words if word.lower() not in stop_words]\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        fil_text = \" \".join(words)\n",
    "        filtered_dataset.append((fil_text, y))\n",
    "    return filtered_dataset# [(text, 0), (text, 1)...] \n",
    "\n",
    "# Specify the path each of the datasets. Point to the directory containing the ham, spam directories - \n",
    "enron1_test = load_dataset('project1_datasets/enron1/test')\n",
    "enron1_train = load_dataset('project1_datasets/enron1 2/train')\n",
    "\n",
    "enron2_test = load_dataset('project1_datasets/test')\n",
    "enron2_train = load_dataset('project1_datasets/train')\n",
    "\n",
    "enron4_test = load_dataset('project1_datasets/enron4/test')\n",
    "enron4_train = load_dataset('project1_datasets/enron4 2/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the text data to bernoulli and bow models\n",
    "def convert_to_bow(dataset):\n",
    "    emails = [email for email, y in dataset]\n",
    "    c = [y for email, y in dataset]\n",
    "\n",
    "    bow_matrix = bow_vectorizer.fit_transform(emails)\n",
    "    return bow_matrix, np.array(c)\n",
    "\n",
    "\n",
    "def convert_to_bernoulli(dataset):\n",
    "    emails = [email for email, y in dataset]\n",
    "    c = [y for email, y in dataset]\n",
    "\n",
    "    brnli_matrix = brnli_vectorizer.fit_transform(emails)\n",
    "    return brnli_matrix, np.array(c)\n",
    "    \n",
    "# Bag Of Words datasets\n",
    "e1_Xtrain_bow, e1_ytrain_bow = convert_to_bow(enron1_train)\n",
    "e1_Xtest_bow, e1_ytest_bow = convert_to_bow(enron1_test)\n",
    "\n",
    "e2_Xtrain_bow, e2_ytrain_bow = convert_to_bow(enron2_train)\n",
    "e2_Xtest_bow, e2_ytest_bow = convert_to_bow(enron2_test)\n",
    "\n",
    "e4_Xtrain_bow, e4_ytrain_bow = convert_to_bow(enron4_train)\n",
    "e4_Xtest_bow, e4_ytest_bow = convert_to_bow(enron4_test)\n",
    "\n",
    "# Bernoulli datasets\n",
    "e1_Xtrain_bli, e1_ytrain_bli = convert_to_bernoulli(enron1_train)\n",
    "e1_Xtest_bli, e1_ytest_bli = convert_to_bernoulli(enron1_test)\n",
    "\n",
    "e2_Xtrain_bli, e2_ytrain_bli = convert_to_bernoulli(enron2_train)\n",
    "e2_Xtest_bli, e2_ytest_bli = convert_to_bernoulli(enron2_test)\n",
    "\n",
    "e4_Xtrain_bli, e4_ytrain_bli = convert_to_bernoulli(enron4_train)\n",
    "e4_Xtest_bli, e4_ytest_bli = convert_to_bernoulli(enron4_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 9370)\n",
      "(456, 10822)\n",
      "450\n"
     ]
    }
   ],
   "source": [
    "print(e1_Xtrain_bow.shape)\n",
    "print(e1_Xtest_bow.shape)\n",
    "print(len(e1_ytrain_bow))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m mbn \u001b[39m=\u001b[39m MultinomialNaiveBayes()\n\u001b[1;32m     40\u001b[0m mbn\u001b[39m.\u001b[39mtrain(e1_Xtrain_bow, e1_ytrain_bow)\n\u001b[0;32m---> 41\u001b[0m predictions \u001b[39m=\u001b[39m mbn\u001b[39m.\u001b[39;49mpredict(e1_Xtest_bow)\n",
      "Cell \u001b[0;32mIn[132], line 34\u001b[0m, in \u001b[0;36mMultinomialNaiveBayes.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     25\u001b[0m     log_prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters[c][\u001b[39m'\u001b[39m\u001b[39mprior\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m     \u001b[39m# common_words = np.isin(word_prob.nonzero()[1], x.nonzero()[1])\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \n\u001b[1;32m     29\u001b[0m     \u001b[39m# x = x[:, common_words]\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[39m# word_prob_common = word_prob[:, common_words]\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m     \u001b[39m# log_prob += np.sum(np.sum(np.multiply(x, word_prob_common)))\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     log_prob \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39msum((np\u001b[39m.\u001b[39;49mdot(x, word_prob))))\n\u001b[1;32m     35\u001b[0m     class_scores\u001b[39m.\u001b[39mappend(log_prob)\n\u001b[1;32m     36\u001b[0m results\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses[np\u001b[39m.\u001b[39margmax(class_scores)]) \n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/scipy/sparse/_base.py:590\u001b[0m, in \u001b[0;36mspmatrix.__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__mul__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m--> 590\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_dispatch(other)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/scipy/sparse/_base.py:536\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_multivector(other)\n\u001b[1;32m    534\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[1;32m    535\u001b[0m     \u001b[39m# scalar value\u001b[39;00m\n\u001b[0;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_scalar(other)\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m issparse(other):\n\u001b[1;32m    539\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m other\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/scipy/sparse/_data.py:124\u001b[0m, in \u001b[0;36m_data_matrix._mul_scalar\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mul_scalar\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_data(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata \u001b[39m*\u001b[39;49m other)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/scipy/sparse/_compressed.py:1230\u001b[0m, in \u001b[0;36m_cs_matrix._with_data\u001b[0;34m(self, data, copy)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a matrix with the same sparsity structure as self,\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m \u001b[39mbut with different data.  By default the structure arrays\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[39m(i.e. .indptr and .indices) are copied.\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[0;32m-> 1230\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m((data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices\u001b[39m.\u001b[39;49mcopy(),\n\u001b[1;32m   1231\u001b[0m                            \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindptr\u001b[39m.\u001b[39;49mcopy()),\n\u001b[1;32m   1232\u001b[0m                           shape\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape,\n\u001b[1;32m   1233\u001b[0m                           dtype\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mdtype)\n\u001b[1;32m   1234\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m((data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindptr),\n\u001b[1;32m   1236\u001b[0m                           shape\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/scipy/sparse/_compressed.py:106\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 106\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_format(full_check\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/scipy/sparse/_compressed.py:157\u001b[0m, in \u001b[0;36m_cs_matrix.check_format\u001b[0;34m(self, full_check)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    154\u001b[0m     warn(\u001b[39m\"\u001b[39m\u001b[39mindices array has non-integer dtype (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname), stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m--> 157\u001b[0m idx_dtype \u001b[39m=\u001b[39m get_index_dtype((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindptr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices))\n\u001b[1;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindptr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindptr, dtype\u001b[39m=\u001b[39midx_dtype)\n\u001b[1;32m    159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices, dtype\u001b[39m=\u001b[39midx_dtype)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/scipy/sparse/_sputils.py:169\u001b[0m, in \u001b[0;36mget_index_dtype\u001b[0;34m(arrays, maxval, check_contents)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_index_dtype\u001b[39m(arrays\u001b[39m=\u001b[39m(), maxval\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_contents\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    148\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39m    Based on input (integer) arrays `a`, determine a suitable index data\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39m    type that can hold the data in the arrays.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     int32min \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mint32(np\u001b[39m.\u001b[39;49miinfo(np\u001b[39m.\u001b[39;49mint32)\u001b[39m.\u001b[39;49mmin)\n\u001b[1;32m    170\u001b[0m     int32max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mint32(np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax)\n\u001b[1;32m    172\u001b[0m     \u001b[39m# not using intc directly due to misinteractions with pythran\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    def train(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = []\n",
    "        self.vocab = np.sum(X, axis = 0)\n",
    "        self.features = X.shape[1]\n",
    "\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            param_c = {}\n",
    "            param_c['prior'] = np.log(X_c.shape[0] / float(X.shape[0]))\n",
    "            param_c[\"word_count\"] = np.sum(X_c, axis = 0)\n",
    "            # print(param_c[\"word_count\"].shape)\n",
    "            # param_c['word_prob'] = np.log((param_c[\"word_count\"] + add_one) / (np.sum(param_c[\"word_count\"]) + X.shape[1]))\n",
    "            param_c['word_prob'] = (param_c[\"word_count\"] + add_one) / (np.sum(param_c[\"word_count\"]) + X.shape[1])\n",
    "            self.parameters.append(param_c)\n",
    "\n",
    "            \n",
    "    def predict(self, X):\n",
    "        results = []\n",
    "        X = X[:, :self.features]\n",
    "        for x in X:\n",
    "            class_scores = []\n",
    "            for c in self.classes:\n",
    "                word_prob = self.parameters[c]['word_prob']\n",
    "                log_prob = self.parameters[c]['prior']\n",
    "                \n",
    "                # common_words = np.isin(word_prob.nonzero()[1], x.nonzero()[1])\n",
    "                \n",
    "                # x = x[:, common_words]\n",
    "                # word_prob_common = word_prob[:, common_words]\n",
    "            \n",
    "                # log_prob += np.sum(np.sum(np.multiply(x, word_prob_common)))\n",
    "                \n",
    "                log_prob += np.sum(x * word_prob)\n",
    "                class_scores.append(log_prob)\n",
    "            results.append(self.classes[np.argmax(class_scores)]) \n",
    "        return np.array(results)\n",
    "\n",
    "mbn = MultinomialNaiveBayes()\n",
    "mbn.train(e1_Xtrain_bow, e1_ytrain_bow)\n",
    "predictions = mbn.predict(e1_Xtest_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 30, 119],\n",
       "       [ 28, 279]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = confusion_matrix(e1_ytest_bow, predictions)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Discrete Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteNaiveBayes:\n",
    "    def train(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.probabilities = {}\n",
    "        self.prior = {}\n",
    "        self.features = X.shape[1]\n",
    "        \n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.prior[c] = np.log(X_c.shape[0] / X.shape[0])\n",
    "            \n",
    "            p = np.log((X_c.sum(axis = 0) + add_one )/ (X_c.shape[0]))\n",
    "            self.probabilities[c] = p\n",
    "\n",
    "    def predict(self, X):\n",
    "        results = []\n",
    "        X = X[:, : self.features]\n",
    "        X = X.toarray()\n",
    "        for x in X: \n",
    "            class_scores = []\n",
    "\n",
    "            for c in self.classes:\n",
    "                p = self.probabilities[c]\n",
    "                prior = self.prior[c]\n",
    "\n",
    "                posterior = prior \n",
    "                posterior += ((x * p[0, c]) + (1-x) * (1 - p[0, c])).sum()\n",
    "                class_scores.append(posterior)\n",
    "            \n",
    "            results.append(self.classes[np.argmax(posterior)])\n",
    "        return results\n",
    "\n",
    "        \n",
    "dnm = DiscreteNaiveBayes()\n",
    "dnm.train(e2_Xtrain_bli, e2_ytrain_bli)\n",
    "pred = dnm.predict(e2_Xtest_bli)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[130,   0],\n",
       "       [348,   0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = confusion_matrix(e2_ytest_bli, pred)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(e1_Xtest_bli))\n",
    "print(type(e1_Xtest_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([1,2,3])\n",
    "np.concatenate(([0], arr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, l2 = 0.1):\n",
    "        self.weights = None\n",
    "        self.num_features = None\n",
    "        self.lambda_ = l2\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, X, y, h):\n",
    "        m = len(y)\n",
    "        return (np.dot(X.T, (y - h)) / m) - ((self.lambda_ * np.concatenate([[0], self.weights[1:]])) / m)\n",
    "\n",
    "    def loss(self, y, h, weights):\n",
    "        m = len(y)\n",
    "        return np.mean(y * np.log(h + 10**-9) + (1 - y) * np.log(1 - h + 10**-9)) - (self.lambda_ * np.sum(weights[1:] ** 2) / 2)\n",
    "\n",
    "    def train(self, X, y, learning_rate = 0.3, num_iter = 10000):\n",
    "        X = X.toarray()\n",
    "        self.num_features = X.shape[1]\n",
    "        X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "\n",
    "        for i in range(num_iter):\n",
    "            z = np.dot(X, self.weights)\n",
    "            pred = self.sigmoid(z)\n",
    "            loss = self.loss(y, pred, self.weights)\n",
    "            grad = self.gradient(X, y, pred)\n",
    "            self.weights += learning_rate * grad # gradient ascent \n",
    "\n",
    "            if (i + 1) % 1000 == 0: \n",
    "                # acc = accuracy_score(y, np.round(pred))\n",
    "                print(f'Loss after {i + 1} iterations: {loss}')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = X.toarray()\n",
    "        X = X[:,:self.num_features]\n",
    "        # X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "\n",
    "        z = np.dot(X, self.weights[1:]) + self.weights[0]\n",
    "        y_pred = self.sigmoid(z)\n",
    "        return np.round(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 1000 iterations: -0.13104265062750026\n",
      "Loss after 2000 iterations: -0.12899023940837484\n",
      "Loss after 3000 iterations: -0.13885696580260642\n",
      "Loss after 4000 iterations: -0.15122087911857335\n",
      "Loss after 5000 iterations: -0.16404688133829948\n",
      "Loss after 6000 iterations: -0.17671563198597148\n",
      "Loss after 7000 iterations: -0.1890174546993992\n",
      "Loss after 8000 iterations: -0.2008871306307122\n",
      "Loss after 9000 iterations: -0.21231576842991626\n",
      "Loss after 10000 iterations: -0.2233171663516595\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(l2 = 0.01)\n",
    "lr.train(e2_Xtrain_bow, e2_ytrain_bow, learning_rate = 0.03, num_iter= 10000)\n",
    "kk = lr.predict(e2_Xtest_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42887029288702927\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc = accuracy_score(e2_ytest_bow, kk)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test splitting and learning lambda"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifer from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "sgd = SGDClassifier(max_iter=10000)\n",
    "\n",
    "param_grid = {\n",
    "    'loss': ['hinge', 'log_loss', 'modified_huber'],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(sgd, param_grid, cv = 5)\n",
    "grid_search.fit(e1_Xtrain_bow, e1_ytrain_bow)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "test_score = best_estimator.score(e1_Xtest_bow[:, :e1_Xtrain_bow.shape[1]], e1_ytest_bow)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47149122807017546"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
