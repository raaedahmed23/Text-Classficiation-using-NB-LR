{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/raaed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/raaed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/raaed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "bow_vectorizer = CountVectorizer()\n",
    "brnli_vectorizer = CountVectorizer(binary = True)\n",
    "\n",
    "add_one = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath) -> list[tuple]:\n",
    "    ham_data = []\n",
    "    spam_data = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(filepath):\n",
    "        for filename in filenames:\n",
    "            with open(os.path.join(dirpath,filename), encoding = 'iso-8859-1') as f:\n",
    "                if 'ham' in filename:\n",
    "                    ham_data.append(f.read())\n",
    "                elif 'spam' in filename:\n",
    "                    spam_data.append(f.read())\n",
    "    \n",
    "    # assign 1/0 to samples and combine data\n",
    "    positive_samples = [(email, 1) for email in ham_data]\n",
    "    negative_samples = [(email, 0) for email in spam_data]\n",
    "\n",
    "    all_samples = positive_samples + negative_samples\n",
    "    random.shuffle(all_samples)\n",
    "\n",
    "    #remove stop words and lemmatize dataset\n",
    "    filtered_dataset = []\n",
    "    for text, y in all_samples:\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words = [word for word in words if word.lower() not in stop_words]\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        fil_text = \" \".join(words)\n",
    "        filtered_dataset.append((fil_text, y))\n",
    "    return filtered_dataset# [(text, 0), (text, 1)...] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path each of the datasets. Point to the directory containing the ham, spam directories - \n",
    "enron1_test = load_dataset('project1_datasets/enron1/test')\n",
    "enron1_train = load_dataset('project1_datasets/enron1 2/train')\n",
    "\n",
    "enron2_test = load_dataset('project1_datasets/test')\n",
    "enron2_train = load_dataset('project1_datasets/train')\n",
    "\n",
    "enron4_test = load_dataset('project1_datasets/enron4/test')\n",
    "enron4_train = load_dataset('project1_datasets/enron4 2/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the text data to bernoulli and bow models\n",
    "def convert_to_bow(dataset):\n",
    "    emails = [email for email, y in dataset]\n",
    "    c = [y for email, y in dataset]\n",
    "\n",
    "    bow_matrix = bow_vectorizer.fit_transform(emails)\n",
    "    return bow_matrix, np.array(c)\n",
    "\n",
    "\n",
    "def convert_to_bernoulli(dataset):\n",
    "    emails = [email for email, y in dataset]\n",
    "    c = [y for email, y in dataset]\n",
    "\n",
    "    brnli_matrix = brnli_vectorizer.fit_transform(emails)\n",
    "    return brnli_matrix, np.array(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag Of Words datasets\n",
    "e1_Xtrain_bow, e1_ytrain_bow = convert_to_bow(enron1_train)\n",
    "e1_Xtest_bow, e1_ytest_bow = convert_to_bow(enron1_test)\n",
    "\n",
    "e2_Xtrain_bow, e2_ytrain_bow = convert_to_bow(enron2_train)\n",
    "e2_Xtest_bow, e2_ytest_bow = convert_to_bow(enron2_test)\n",
    "\n",
    "e4_Xtrain_bow, e4_ytrain_bow = convert_to_bow(enron4_train)\n",
    "e4_Xtest_bow, e4_ytest_bow = convert_to_bow(enron4_test)\n",
    "\n",
    "# Bernoulli datasets\n",
    "e1_Xtrain_bli, e1_ytrain_bli = convert_to_bernoulli(enron1_train)\n",
    "e1_Xtest_bli, e1_ytest_bli = convert_to_bernoulli(enron1_test)\n",
    "\n",
    "e2_Xtrain_bli, e2_ytrain_bli = convert_to_bernoulli(enron2_train)\n",
    "e2_Xtest_bli, e2_ytest_bli = convert_to_bernoulli(enron2_test)\n",
    "\n",
    "e4_Xtrain_bli, e4_ytrain_bli = convert_to_bernoulli(enron4_train)\n",
    "e4_Xtest_bli, e4_ytest_bli = convert_to_bernoulli(enron4_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    def train(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = []\n",
    "        self.vocab = np.sum(X, axis = 0)\n",
    "        self.features = X.shape[1]\n",
    "\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            param_c = {}\n",
    "            param_c['prior'] = np.log(X_c.shape[0] / float(X.shape[0]))\n",
    "            param_c[\"word_count\"] = np.sum(X_c, axis = 0)\n",
    "            param_c['word_prob'] = np.log((param_c[\"word_count\"] + add_one) / (np.sum(param_c[\"word_count\"]) + X.shape[1]))\n",
    "            self.parameters.append(param_c)\n",
    "\n",
    "            \n",
    "    def predict(self, X):\n",
    "        results = []\n",
    "        for x in X:\n",
    "            class_scores = []\n",
    "            for c in self.classes:\n",
    "                word_prob = self.parameters[c]['word_prob']\n",
    "                log_prob = self.parameters[c]['prior']\n",
    "                \n",
    "                common_words = np.isin(word_prob.nonzero()[1], x.nonzero()[1])\n",
    "                \n",
    "                x = x[:, common_words]\n",
    "                word_prob_common = word_prob[:, common_words]\n",
    "            \n",
    "                log_prob += np.sum(np.sum(np.multiply(x, word_prob_common)))\n",
    "                class_scores.append(log_prob)\n",
    "            results.append(self.classes[np.argmax(class_scores)]) \n",
    "        return np.array(results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on Bag of Words Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnb_evaluate(X_train, y_train, X_test, y_test, name):\n",
    "    mbn = MultinomialNaiveBayes()\n",
    "    mbn.train(X_train, y_train)\n",
    "    predictions = mbn.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "\n",
    "    print(f'Multinomial Naive Bayes Results on {name} Dataset')\n",
    "    print(f'Accuracy = {acc}')\n",
    "    print(f'Precision = {precision}')\n",
    "    print(f'Recall = {recall}')\n",
    "    print(f'F1 score = {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Results on Enron 1 Dataset\n",
      "Accuracy = 0.6776315789473685\n",
      "Precision = 0.7010050251256281\n",
      "Recall = 0.9087947882736156\n",
      "F1 score = 0.7914893617021277\n"
     ]
    }
   ],
   "source": [
    "mnb_evaluate(e1_Xtrain_bow, e1_ytrain_bow, e1_Xtest_bow, e1_ytest_bow, 'Enron 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Results on Enron 2 Dataset\n",
      "Accuracy = 0.7238493723849372\n",
      "Precision = 0.7523364485981309\n",
      "Recall = 0.9252873563218391\n",
      "F1 score = 0.8298969072164949\n"
     ]
    }
   ],
   "source": [
    "mnb_evaluate(e2_Xtrain_bow, e2_ytrain_bow, e2_Xtest_bow, e2_ytest_bow, 'Enron 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Results on Enron 4 Dataset\n",
      "Accuracy = 0.27992633517495397\n",
      "Precision = 0.27992633517495397\n",
      "Recall = 1.0\n",
      "F1 score = 0.4374100719424461\n"
     ]
    }
   ],
   "source": [
    "mnb_evaluate(e4_Xtrain_bow, e4_ytrain_bow, e4_Xtest_bow, e4_ytest_bow, 'Enron 4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Discrete Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 149]\n",
      " [  0 307]]\n"
     ]
    }
   ],
   "source": [
    "class DiscreteNaiveBayes:\n",
    "    def train(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.probabilities = {}\n",
    "        self.prior = {}\n",
    "        self.features = X.shape[1]\n",
    "        \n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.prior[c] = np.log(X_c.shape[0] / X.shape[0])\n",
    "            \n",
    "            p = np.log((X_c.sum(axis = 0) + add_one )/ (X_c.sum() + 2))\n",
    "            self.probabilities[c] = p\n",
    "\n",
    "    def predict(self, X):\n",
    "        results = []\n",
    "        X = X[:, : self.features]\n",
    "        X = X.toarray()\n",
    "        for x in X: \n",
    "            class_scores = []\n",
    "            for c in self.classes:\n",
    "                p = self.probabilities[c]\n",
    "                prior = self.prior[c]\n",
    "                p = np.asarray(p).flatten()\n",
    "                if len(p) > len(x):\n",
    "                    diff = len(p) - len(x)\n",
    "                    x = np.pad(x, (0, diff), 'constant')\n",
    "                posterior = prior + ((x * p) + ((1 - x) * (1 - p))).sum()\n",
    "                class_scores.append(posterior)\n",
    "                     \n",
    "            results.append(self.classes[np.argmax(class_scores)])\n",
    "        return results\n",
    "\n",
    "dnb = DiscreteNaiveBayes()\n",
    "dnb.train(e1_Xtrain_bli, e1_ytrain_bli)\n",
    "pred = dnb.predict(e1_Xtest_bli)\n",
    "\n",
    "matrix = confusion_matrix(e1_ytest_bli, pred)\n",
    "print(matrix)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on Bernoulli Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnb_evaluate(X_train, y_train, X_test, y_test, name):\n",
    "    dnb = DiscreteNaiveBayes()\n",
    "    dnb.train(X_train, y_train)\n",
    "    predictions = dnb.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "\n",
    "    print(f'Discrete Naive Bayes Results on {name} Dataset')\n",
    "    print(f'Accuracy = {acc}')\n",
    "    print(f'Precision = {precision}')\n",
    "    print(f'Recall = {recall}')\n",
    "    print(f'F1 score = {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete Naive Bayes Results on Enron 1 Dataset\n",
      "Accuracy = 0.6732456140350878\n",
      "Precision = 0.6732456140350878\n",
      "Recall = 1.0\n",
      "F1 score = 0.8047182175622543\n"
     ]
    }
   ],
   "source": [
    "dnb_evaluate(e1_Xtrain_bli, e1_ytrain_bli, e1_Xtest_bli, e1_ytest_bli, 'Enron 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete Naive Bayes Results on Enron 2 Dataset\n",
      "Accuracy = 0.7280334728033473\n",
      "Precision = 0.7280334728033473\n",
      "Recall = 1.0\n",
      "F1 score = 0.8426150121065376\n"
     ]
    }
   ],
   "source": [
    "dnb_evaluate(e2_Xtrain_bli, e2_ytrain_bli, e2_Xtest_bli, e2_ytest_bli, 'Enron 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete Naive Bayes Results on Enron 4 Dataset\n",
      "Accuracy = 0.7200736648250461\n",
      "Precision = 0.0\n",
      "Recall = 0.0\n",
      "F1 score = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "dnb_evaluate(e4_Xtrain_bli, e4_ytrain_bli, e4_Xtest_bli, e4_ytest_bli, 'Enron 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, l2 = 0.1):\n",
    "        self.weights = None\n",
    "        self.num_features = None\n",
    "        self.lambda_ = l2\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, X, y, h):\n",
    "        m = len(y)\n",
    "        return (np.dot(X.T, (y - h)) / m) - ((self.lambda_ * np.concatenate([[0], self.weights[1:]])) / m)\n",
    "\n",
    "    def loss(self, y, h, weights):\n",
    "        m = len(y)\n",
    "        return np.mean(y * np.log(h + 10**-9) + (1 - y) * np.log(1 - h + 10**-9)) - (self.lambda_ * np.sum(weights[1:] ** 2) / 2)\n",
    "\n",
    "    def train(self, X, y, learning_rate = 0.01, max_iter = 10000):\n",
    "        X = X.toarray()\n",
    "        self.num_features = X.shape[1]\n",
    "        X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            z = np.dot(X, self.weights)\n",
    "            pred = self.sigmoid(z)\n",
    "            loss = self.loss(y, pred, self.weights)\n",
    "            grad = self.gradient(X, y, pred)\n",
    "            self.weights += learning_rate * grad # gradient ascent \n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = X.toarray()\n",
    "        X = X[:,:self.num_features]\n",
    "\n",
    "        if X.shape[1] < len(self.weights):\n",
    "            self.weights = self.weights[:X.shape[1]+1]\n",
    "        z = np.dot(X, self.weights[1:]) + self.weights[0]\n",
    "        y_pred = self.sigmoid(z)\n",
    "        return np.round(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [0.001, 0.01, 0.1, 1]\n",
    "def cross_validation(X, y, grid):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 42) # 70/30 train validation split\n",
    "    best_f1 = 0 \n",
    "    lambda_ = None\n",
    "    \n",
    "    for l in grid: # train LR using each value of lambda\n",
    "        lr = LogisticRegression(l2 = l) \n",
    "        lr.train(X_train, y_train, max_iter = 5000)\n",
    "        pred = lr.predict(X_val)\n",
    "\n",
    "        f1 = f1_score(y_val, pred)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            lambda_ = l\n",
    "\n",
    "    return lambda_\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_evaluate(X_train, y_train, X_test, y_test, name):\n",
    "    lambda_ = cross_validation(X_train, y_train, grid)\n",
    "\n",
    "    lr = LogisticRegression(l2 = lambda_)\n",
    "    lr.train(X_train, y_train, max_iter = 10000)\n",
    "    predictions = lr.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "\n",
    "    print(f'Logistic Regresstion Results on {name} Dataset')\n",
    "    print(f'Accuracy = {acc}')\n",
    "    print(f'Precision = {precision}')\n",
    "    print(f'Recall = {recall}')\n",
    "    print(f'F1 score = {f1}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regresstion Results on Enron 1 BOW Dataset\n",
      "Accuracy = 0.43201754385964913\n",
      "Precision = 0.7727272727272727\n",
      "Recall = 0.22149837133550487\n",
      "F1 score = 0.34430379746835443\n"
     ]
    }
   ],
   "source": [
    "lr_evaluate(e1_Xtrain_bow, e1_ytrain_bow, e1_Xtest_bow, e1_ytest_bow, 'Enron 1 BOW') # approx 30 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regresstion Results on Enron 1 Bernoulli Dataset\n",
      "Accuracy = 0.41228070175438597\n",
      "Precision = 0.7407407407407407\n",
      "Recall = 0.19543973941368079\n",
      "F1 score = 0.3092783505154639\n"
     ]
    }
   ],
   "source": [
    "lr_evaluate(e1_Xtrain_bli, e1_ytrain_bli, e1_Xtest_bli, e1_ytest_bli, 'Enron 1 Bernoulli') # approx 30 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regresstion Results on Enron 2 BOW Dataset\n",
      "Accuracy = 0.4497907949790795\n",
      "Precision = 0.8102189781021898\n",
      "Recall = 0.31896551724137934\n",
      "F1 score = 0.45773195876288664\n"
     ]
    }
   ],
   "source": [
    "lr_evaluate(e2_Xtrain_bow, e2_ytrain_bow, e2_Xtest_bow, e2_ytest_bow, 'Enron 2 BOW') # approx 35 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regresstion Results on Enron 2 Bernoulli Dataset\n",
      "Accuracy = 0.5209205020920502\n",
      "Precision = 0.7741935483870968\n",
      "Recall = 0.4827586206896552\n",
      "F1 score = 0.5946902654867257\n"
     ]
    }
   ],
   "source": [
    "lr_evaluate(e2_Xtrain_bli, e2_ytrain_bli, e2_Xtest_bli, e2_ytest_bli, 'Enron 2 Bernoulli') # approx 35 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regresstion Results on Enron 4 BOW Dataset\n",
      "Accuracy = 0.7182320441988951\n",
      "Precision = 0.0\n",
      "Recall = 0.0\n",
      "F1 score = 0.0\n"
     ]
    }
   ],
   "source": [
    "lr_evaluate(e4_Xtrain_bow, e4_ytrain_bow, e4_Xtest_bow, e4_ytest_bow, 'Enron 4 BOW') # approx 1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regresstion Results on Enron 4 Bernoulli Dataset\n",
      "Accuracy = 0.7200736648250461\n",
      "Precision = 0.0\n",
      "Recall = 0.0\n",
      "F1 score = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "lr_evaluate(e4_Xtrain_bli, e4_ytrain_bli, e4_Xtest_bli, e4_ytest_bli, 'Enron 4 Bernoulli') # approx 1 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifer from scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def sgd_eval(X_train, y_train, X_test, y_test, name):\n",
    "    sgd = SGDClassifier(max_iter=5000)\n",
    "\n",
    "    param_grid = {\n",
    "        'loss': ['hinge', 'log_loss', 'modified_huber'],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(sgd, param_grid, cv = 5)\n",
    "    grid_search.fit(e1_Xtrain_bow, e1_ytrain_bow)\n",
    "\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "\n",
    "    X_test.resize((X_test.shape[0], X_train.shape[1]))\n",
    "    print(X_test.shape)\n",
    "\n",
    "    predictions = best_estimator.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "\n",
    "    print(f'Logistic Regresstion Results on {name} Dataset')\n",
    "    print(f'Accuracy = {acc}')\n",
    "    print(f'Precision = {precision}')\n",
    "    print(f'Recall = {recall}')\n",
    "    print(f'F1 score = {f1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regresstion Results on Enron 1 BOW Dataset\n",
      "Accuracy = 0.45614035087719296\n",
      "Precision = 0.7610619469026548\n",
      "Recall = 0.28013029315960913\n",
      "F1 score = 0.4095238095238095\n"
     ]
    }
   ],
   "source": [
    "sgd_eval(e1_Xtrain_bow, e1_ytrain_bow, e1_Xtest_bow, e1_ytest_bow, 'Enron 1 BOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regresstion Results on Enron 1 Bernoulli Dataset\n",
      "Accuracy = 0.45614035087719296\n",
      "Precision = 0.7185185185185186\n",
      "Recall = 0.31596091205211724\n",
      "F1 score = 0.43891402714932126\n"
     ]
    }
   ],
   "source": [
    "sgd_eval(e1_Xtrain_bli, e1_ytrain_bli, e1_Xtest_bli, e1_ytest_bli, 'Enron 1 Bernoulli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 9682 features, but SGDClassifier is expecting 9370 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sgd_eval(e2_Xtrain_bow, e2_ytrain_bow, e2_Xtest_bow, e2_ytest_bow, \u001b[39m'\u001b[39;49m\u001b[39mEnron 2 BOW\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[112], line 20\u001b[0m, in \u001b[0;36msgd_eval\u001b[0;34m(X_train, y_train, X_test, y_test, name)\u001b[0m\n\u001b[1;32m     16\u001b[0m best_estimator \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_estimator_\n\u001b[1;32m     18\u001b[0m X_test\u001b[39m.\u001b[39mresize((X_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], X_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n\u001b[0;32m---> 20\u001b[0m predictions \u001b[39m=\u001b[39m best_estimator\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[1;32m     22\u001b[0m acc \u001b[39m=\u001b[39m accuracy_score(y_test, predictions)\n\u001b[1;32m     23\u001b[0m f1 \u001b[39m=\u001b[39m f1_score(y_test, predictions)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_base.py:419\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39mPredict class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 419\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(X)\n\u001b[1;32m    420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(scores\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    421\u001b[0m     indices \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(scores \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mint\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_base.py:400\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    397\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    398\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 400\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    401\u001b[0m scores \u001b[39m=\u001b[39m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[1;32m    402\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39mreshape(scores, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m scores\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m scores\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/base.py:569\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 569\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    571\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.10/site-packages/sklearn/base.py:370\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 370\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 9682 features, but SGDClassifier is expecting 9370 features as input."
     ]
    }
   ],
   "source": [
    "sgd_eval(e2_Xtrain_bow, e2_ytrain_bow, e2_Xtest_bow, e2_ytest_bow, 'Enron 2 BOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
