{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/raaed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/raaed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/raaed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 1]), array([1, 0, 2]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.array([[0, 1, 0], [2, 0, 3]])\n",
    "result = arr.nonzero()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "bow_vectorizer = CountVectorizer()\n",
    "brnli_vectorizer = CountVectorizer(binary = True)\n",
    "add_one = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath) -> list[tuple]:\n",
    "    ham_data = []\n",
    "    spam_data = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(filepath):\n",
    "        for filename in filenames:\n",
    "            with open(os.path.join(dirpath,filename), encoding = 'iso-8859-1') as f:\n",
    "                if 'ham' in filename:\n",
    "                    ham_data.append(f.read())\n",
    "                elif 'spam' in filename:\n",
    "                    spam_data.append(f.read())\n",
    "    \n",
    "    # assign 1/0 to samples and combine data\n",
    "    positive_samples = [(email, 1) for email in ham_data]\n",
    "    negative_samples = [(email, 0) for email in spam_data]\n",
    "\n",
    "    all_samples = positive_samples + negative_samples\n",
    "    random.shuffle(all_samples)\n",
    "\n",
    "    #remove stop words and lemmatize dataset\n",
    "    filtered_dataset = []\n",
    "    for text, y in all_samples:\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words = [word for word in words if word.lower() not in stop_words]\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        fil_text = \" \".join(words)\n",
    "        filtered_dataset.append((fil_text, y))\n",
    "    return filtered_dataset# [(text, 0), (text, 1)...] \n",
    "\n",
    "# Specify the path each of the datasets. Point to the directory containing the ham, spam directories - \n",
    "enron1_test = load_dataset('project1_datasets/enron1/test')\n",
    "enron1_train = load_dataset('project1_datasets/enron1 2/train')\n",
    "\n",
    "enron2_test = load_dataset('project1_datasets/test')\n",
    "enron2_train = load_dataset('project1_datasets/train')\n",
    "\n",
    "enron4_test = load_dataset('project1_datasets/enron4/test')\n",
    "enron4_train = load_dataset('project1_datasets/enron4 2/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the text data to bernoulli and bow models\n",
    "def convert_to_bow(dataset):\n",
    "    emails = [email for email, y in dataset]\n",
    "    c = [y for email, y in dataset]\n",
    "\n",
    "    bow_matrix = bow_vectorizer.fit_transform(emails)\n",
    "    return bow_matrix, np.array(c)\n",
    "\n",
    "\n",
    "def convert_to_bernoulli(dataset):\n",
    "    emails = [email for email, y in dataset]\n",
    "    c = [y for email, y in dataset]\n",
    "\n",
    "    brnli_matrix = brnli_vectorizer.fit_transform(emails)\n",
    "    return brnli_matrix, np.array(c)\n",
    "    \n",
    "# Bag Of Words datasets\n",
    "e1_Xtrain_bow, e1_ytrain_bow = convert_to_bow(enron1_train)\n",
    "e1_Xtest_bow, e1_ytest_bow = convert_to_bow(enron1_test)\n",
    "\n",
    "e2_Xtrain_bow, e2_ytrain_bow = convert_to_bow(enron2_train)\n",
    "e2_Xtest_bow, e2_ytest_bow = convert_to_bow(enron2_test)\n",
    "\n",
    "e4_Xtrain_bow, e4_ytrain_bow = convert_to_bow(enron4_train)\n",
    "e4_Xtest_bow, e4_ytest_bow = convert_to_bow(enron4_test)\n",
    "\n",
    "# Bernoulli datasets\n",
    "e1_Xtrain_bli, e1_ytrain_bli = convert_to_bernoulli(enron1_train)\n",
    "e1_Xtest_bli, e1_ytest_bli = convert_to_bernoulli(enron1_test)\n",
    "\n",
    "e2_Xtrain_bli, e2_ytrain_bli = convert_to_bernoulli(enron2_train)\n",
    "e2_Xtest_bli, e2_ytest_bli = convert_to_bernoulli(enron2_test)\n",
    "\n",
    "e4_Xtrain_bli, e4_ytrain_bli = convert_to_bernoulli(enron4_train)\n",
    "e4_Xtest_bli, e4_ytest_bli = convert_to_bernoulli(enron4_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 9370)\n",
      "(456, 10822)\n",
      "450\n"
     ]
    }
   ],
   "source": [
    "print(e1_Xtrain_bow.shape)\n",
    "print(e1_Xtest_bow.shape)\n",
    "print(len(e1_ytrain_bow))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    def train(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = []\n",
    "        self.vocab = np.sum(X, axis = 0)\n",
    "\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            param_c = {}\n",
    "            param_c['prior'] = np.log(X_c.shape[0] / float(X.shape[0]))\n",
    "            param_c[\"word_count\"] = np.sum(X_c, axis = 0)\n",
    "            # print(param_c[\"word_count\"].shape)\n",
    "            param_c['word_prob'] = np.log((param_c[\"word_count\"] + add_one) / (np.sum(param_c[\"word_count\"]) + X.shape[1]))\n",
    "            self.parameters.append(param_c)\n",
    "\n",
    "            \n",
    "    def predict(self, X):\n",
    "        results = []\n",
    "        for x in X:\n",
    "            class_scores = []\n",
    "            # x = x.toarray()\n",
    "            for c in self.classes:\n",
    "                word_prob = self.parameters[c]['word_prob']\n",
    "                log_prob = self.parameters[c]['prior']\n",
    "                \n",
    "                common_words = np.isin(word_prob.nonzero()[1], x.nonzero()[1])\n",
    "                x = x[:, common_words]\n",
    "                word_prob_common = word_prob[:, common_words]\n",
    "            \n",
    "                log_prob += np.sum(np.sum(np.multiply(x, word_prob_common)))\n",
    "                class_scores.append(log_prob)\n",
    "            results.append(self.classes[np.argmax(class_scores)]) \n",
    "        return np.array(results)\n",
    "\n",
    "mbn = MultinomialNaiveBayes()\n",
    "mbn.train(e1_Xtrain_bow, e1_ytrain_bow)\n",
    "predictions = mbn.predict(e1_Xtest_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 30, 119],\n",
       "       [ 28, 279]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(predictions.tolist()), type(e1_ytest_bow.tolist()))\n",
    "matrix = confusion_matrix(e1_ytest_bow, predictions)\n",
    "matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Discrete Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
